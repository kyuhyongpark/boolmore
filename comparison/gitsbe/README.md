# Installation

## Install Gitsbe

See https://druglogics.github.io/druglogics-doc/gitsbe-install.html to install gitsbe.
Note that the installation didn't work on Windows.
The solution given below worked with Ubuntu on a Windows subsystem for linux (WSL).

1. install JDK
    `sudo apt install openjdk-21-jdk-headless`
2. install maven
    `sudo apt-get install maven`
3. add JAVA_HOME
    add following line to .profile at home
    `export JAVA_HOME="/usr/lib/jvm/java-21-openjdk-amd64"`
4. install Gitsbe
    ```
    git clone https://github.com/druglogics/gitsbe.git
    cd gitsbe
    mvn clean install
    ```
5. test run
    ```
    cd example_run_ags
    java -cp ../target/gitsbe-1.3.1-jar-with-dependencies.jar eu.druglogics.gitsbe.Launcher --project=test --network=toy_ags_network.sif --trainingdata=toy_ags_training_data.tab --config=toy_ags_config.tab --modeloutputs=toy_ags_modeloutputs.tab
    ```

## (Optional) Install mpbn

Using mpbn to calculate trapspaces offers a significant speedup when the network size is large. For our case study, however, the default method of Gitsbe was significantly faster.

1. install mpbn
    ```
    sudo apt update
    sudo apt install build-essential
    sudo apt-get install python3-dev

    pip install mpbn
    ```

    Download `mpbn-attractors.py` and `model.bnet` from https://github.com/druglogics/druglogics-dep and test run
    ```
    wget https://raw.githubusercontent.com/druglogics/druglogics-dep/refs/heads/master/mpbn-attractors.py
    wget https://raw.githubusercontent.com/druglogics/druglogics-dep/refs/heads/master/model.bnet
    
    python mpbn-attractors.py model.bnet
    ```

2. allow Gitsbe to access mpbn
    See https://github.com/druglogics/druglogics-dep#readme.

    For example, in file `home/user_name/.profile`, add
    ```
    export MPBN_HOME=/pathTo/dir-that-has-the-mpbn-attractors.py
    ```
    so that mpbn-attractors.py is accessible at the environment variable MPBN_HOME

# ABA case study

## Convert ABA case study data

Run `conversion.ipynb`. This converts the baseline start model into a list of edges in sif format. It also adds all extra edges specified in the json file to the list. Note that we cannot enforce constraints nor distinguish the original edges from the extra edges.
Since an interaction graph cannot distinguish source nodes from constant nodes, Gitsbe considers all constant nodes as source nodes. The ABA model has 22 constant nodes, and considering them as source nodes makes it very difficult to enumerate all attractors. For this reason, we substitute the constant node values to other functions (aka percolate the constant values). This eliminates 24 nodes (22 original constants and 2 additional nodes fixed by percolating constant node values).
The result is stored as `ABA_A_network.sif`. We note that certain node names were changed since Gitsbe does not allow certain characters in node names.

`conversion.ipynb` also converts the input of boolmore into training data for Gitsbe. Source node values and perturbations are added as `Condition`. Experiments that have the same perturbation conditions are grouped together for faster computation. The observation categories "OFF", "OFF/Some", "Some", "Some/ON", "ON" are assigned thee values 0, 0.25, 0.5, 0.75, 1.0, respectively as `Response`. Each `Condition`-`Response` pair is assigned a weight equal to the number of observations so that each observation is treated equally.

For example,

| ID | Score |   Source  |        Perturbation        | Observed node | Categorization |
| -- | ----- | --------- | -------------------------- | ------------- | -------------- |
|  1 | 	1.0  |  Source=0 | Mediator1 KO, Mediator2 CA |         Sink1 |             ON |
|  2 |  1.0  |  Source=0 | Mediator1 KO, Mediator2 CA |         Sink2 |       OFF/Some |

will be converted to

```
Condition
Source:0	Mediator1:0	Mediator2:1
Response
Sink1:1	Sink2:0.25
Weight:2
```

Note that experiments that consider the 24 nodes that were eliminated due to percolation are ignored. The result is stored as `ABA_A_training_data.tab`. 

Finally, `conversion.ipynb` also creates `ABA_A_gitsbe.tsv`, which is an experimental data file used to score the models generated by Gitsbe using the boolmore hierarchy scoring method. It omits experiments that have the constant nodes perturbed or observed. It also has certain node names modified.


## Running Gitsbe on ABA case study

Create directory `ABA` in the Gitsbe repository, and copy
```
ABA_A_network.sif
ABA_A_training_data.tab
ABA_config.tab
ABA_modeloutputs.tab
```
And run,
```
java -cp ../target/gitsbe-1.3.1-jar-with-dependencies.jar eu.druglogics.gitsbe.Launcher --project=ABA_A --network=ABA_A_network.sif --trainingdata=ABA_A_training_data.tab --config=ABA_config.tab --modeloutputs=ABA_modeloutputs.tab
```

This runs 6 simulations in parallel, where each simulation consists of 400 generations with 400 models per generation.
Each simulation generates 3 best models, resulting in 18 models.

## Results

The result of a Gitsbe run is in the `ABA_A_20241105_214621` directory.

The whole process took ~13 hours. We score the models using the method of boolmore, based on the 429 experimental results that Gitsbe could process.

The 18 Gitsbe models had a maximum score of 305.0/429 (71.1%) and a maximum attractor agreement (non-hierarchy score) of 343.7/429 (80.1%).  
The 15 boolmore models had a lowest score of 334.3/429 (77.9%) and a lowest attactor agreement (non-hierarchy score) of 359.3/429 (83.8%).

Also note that the Gitsbe models did not follow any biological constraints, as they cannot be specified in Gitsbe.
Most seriously, all 18 models had the function `Closure, Microtubule_Depolymerization`, which means that stomatal closure could be achieved by microtubule depolymerization alone, in the absence of water efflux. This function is biologically unrealistic, as water efflux determines the deflation of the guard cells, without which stomatal closure is impossible.

See `compare.ipynb` to reproduce the comparison results.
